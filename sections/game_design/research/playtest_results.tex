Playtesting was vitally important to our success early on. Knowing that the main goal of our project was to fill the gap where other similar games fell short, we conducted playtesting sessions where we observed volunteers interacting with these games and took notes. By focusing on participants that had little to no coding experience, we were able to examine how the target demographic for our project interacted with material that was geared towards programmatic thinking. Because we were drawing inspiration from these games for our own project, we wanted to know precisely what about them was effective. Similarly, since it is our goal to improve upon the current offerings, we were particularly interested in which aspects of these games were confusing, cumbersome, or overwhelming to inexperienced players.\\

\paragraph{Similar Games}\mbox{} \\
\textit{Human Resource Machine} was the original inspiration for our project, and is well liked by programmers for its ability to have users explore algorithmic design and computational logic. It presents puzzles to the user and gives them a set of simple instructions to build a solution out of and an on screen character acts out each instruction as the sequence of the solution is run through. However, the game does not provide much in the way of informing users about the concepts presented. Rather, it simply gives the users new commands as they progress through the levels and expects them to figure out the mechanics associated with each new command through trial and error. There is also a notable lack of useful information in regards to how they should use each instruction and what its intended purpose is. There is no tutorial element of any sort, and many of the early levels' ``hint" options are completely useless.\\

In preliminary testing, users struggled to successfully navigate the beginner level puzzles, and none of the participants were able to complete level four. The lack of information when new instructions were presented made it very difficult for the users to understand precisely what they needed to achieve in order to successfully complete each puzzle. The users that did seek out hints from the game were met with useless text that did nothing to explain the puzzle, the commands available, or a tactic they might want to explore to nudge them towards a solution. As new instructions were made available to the users on each subsequent level, their frustration with not understanding how the commands worked together compounded and detracted from their desire to continue progressing through the game. The jump command and the registers proved to be very difficult for the users to understand without any effective messaging, and because they are such an integral part of the game, this lack of understanding early on caused a serious roadblock.\\

Several mechanics within the game also contributed to the difficulty that users were experiencing with understanding what they were supposed to be doing, or how they could improve when their solutions didn’t work. A slider bar that increases the speed of the character acting out the instructions allows users to quickly run through their solution. One user turned the slider to max speed while they were still trying to form a solution and this took away their ability to understand where their program was going wrong. In a similar vein, all of the users were focused on the actor and what it was doing to try to figure out the issues with their solution instead of paying attention to the sequence of the solution itself and how to trace through and debug their problems. The debugging tools within the game are never explained, and most users never even attempted to utilize them. The game also allowed users to try the same incorrect solution many times without providing any kind of feedback, and even allowed one player to get stuck in an infinite loop from a poorly placed jump command.\\

In subsequent testing sessions, we provided missing information for users where the game does an insufficient job of providing details. We explained each command, what it does when used, how to manipulate the commands effectively, and how to work with the registers in the game. We also showed the debugging tools to the users and explained how it could be useful to trace through their solution when they encounter problems. Additionally, when users wanted a hint on the level, we provided them with clarifying information about the purpose of the puzzle and what their goal was, as well as small solution based hints, such as “you need multiple jumps in this one” and similar suggestions that nudged them toward a solution without giving away the puzzle. During these sessions, there was a noticeable increase in the comfort level the users had with manipulating the game, especially concerning the debugging tools and manipulating the commands within the solution box. There was also a definitive improvement in users’ ability to successfully solve the puzzles they were presented with.\\

The main take-away from \textit{Human Resource Machine} is that messaging is incredibly important in making a game like this effective. It is very clear that new programmers need more guided instruction to be able to engage in the puzzles without being directed by someone who more clearly understands what to do. By improving the level of information provided, we were able to give users greater confidence in the skills they were learning, and in turn they were both more willing and more capable at solving various puzzles.\\

\textit{7 Billion Humans} is the sequel to \textit{Human Resource Machine}.  It features a much more advanced instruction set and more varied puzzles, but it is plagued by the same communication issues present in its predecessor. The puzzles in this installment were less abstract than before, but the added concreteness came at a severe cost. While the game is highly engaging for those with programming experience, new players were even more lost than they had been previously. Aside from the instructions being much more complex, the puzzles now required the consideration of spatial manipulation of the on screen characters in order to be solved correctly. So, in addition to all of the same difficulties that we saw users struggle with during \textit{Human Resource Machine}, we also observed that users were not retaining information or strategies from puzzle to puzzle.\\

A key observation elucidated from \textit{7 Billion Humans} is that programming puzzle games that rely on spatial manipulation of the actor executing the instructions distract the player from the essence of the puzzle. This is incredibly important because many games in the genre rely on this mechanic. Players frequently became more concerned with counting spaces than understanding the logic needed to solve the puzzle at hand. This space counting led to an interesting approach to the puzzles we did not observe during playtests of \textit{Human Resource Machine}. The spatial puzzle format saw an alarming rise in brute force solutions. By counting squares and adding enough if statements, players could eventually arrive at a solution that was complicated enough to pass -- sometimes on accident. The idea that this tactic was successful was very shocking, as it effectively side-stepped the entire point of the game.\\

\paragraph{Our Prototypes}\mbox{} \\
After we formed the initial versions of our individual prototypes, we had participants play through simple puzzles on them to test out their efficacy. Our prototypes were formed after the playtesting mentioned above, so we were aware of certain shortcomings in these types of games and made efforts to avoid similar pitfalls in our own designs.\\

A problem that we initially ran into was that the paper medium of our prototypes were not as immediately accessible to users. Some individuals struggled with the concepts and didn't seem as confident to begin the playtesting without receiving ample instructions first. We felt that this coincided with our previous findings in the playtesting of other games, but were wary about making a game that was heavily text-based. Even this very first step in the process informed a lot about our game design. We must be able to effectively communicate the gist of our game without losing our players interest in a wall of text.\\

Another interesting result of the playtesting of our early prototypes was that, once the game had been set up for them, participants were more engaged in the prototypes than they had been in the other games. During game playtesting, most users continually sought help or approval from us when we were merely operating in an observational capacity. During the prototype playtesting, though, users seemed much more confident to adjust their solutions and explore the puzzle on their own. It is unclear if the mode of delivery swayed these interactions. For one, the prototypes provided a much more hands-on experience, and therefore were inherently more engaging. Additionally, for the prototype playtests each of us filled the role of the Actor character and were responsible for completing the puzzle manipulations as we processed the participants' solutions. Because we were now essentially part of the game instead of a liason for the game, users may have subconciously avoided trying to ask for help from us as frequently as before.\\

After this first phase of prototype testing, we accumulated all of our individual results and designed a unified prototype. This prototype was a whiteboard-based version of the game that could be easily used and reused for playtesting. It came with a set of sample puzzles that were designed to slowly increase in complexity and make users comfortable with the progression of the game, and also featured the instantiation of our ``data structures represented with cards" mechanic. For reference, we each had a list of the instruction set with short descriptions of what each command was used for.\\

This round of playtesting was extremely informative for us as it was a resounding success. First, the idea of using cards for the data structures worked very well with our users. By using a medium they were familiar with, it was instinctual for them to understand that they had to ``play" a data structure in the puzzle to be able to use it in their solutions. Secondly, the progression of our puzzles provided a concrete base for users to understand the concepts of each puzzle and apply their solutions to similar but more complex puzzles. Also, the progression of the puzzles was found to be steady enough that users found the problems challenging, but not so disjointedly complex that they felt overwhelmed or lost when seeking out a solution. Most users were able to complete the sequence admirably. Finally, we found that our limited instruction set was extremely effective. Users were able to understand the instructions clearly and utilize them effectively for various concepts during their testing.\\

\paragraph{Early Iterations}\mbox{} \\
Implementing our vision in Unity was a very involved and lengthy process. Many systems had to be designed to support the construction of our game, as well as solid foundations for the individual systems. We committed ourselves to realizing the design we had in mind and doing what it took to bring it to life instead of compromising on design ideas when they were found to be much more involved to implement than we had originally planned. Still, we wanted to be sure that our efforts were moving in the right direction, and that we weren't just adhering to what we thought the game should be if the methods were not effective in the actual execution. We continued to playtest our game at different intervals to get continuous feedback to update our goals for change and polish needed in our execution.\\

Many early findings reported things we expected, like that a particular game mechanic was not explained well enough or that the appearance of certain elements in the scene did not stand out enough. This kind of feedback helped reassure us that we were headed in the right direction with our end goal designs. The mechanic that was not explained well enough still had a tutorial that needed to be created for it. The elements in the scene that did not stand out well enough were awaiting replacement art from our artist team. A lot of other feedback we received, though, was not exactly what we were expecting -- people thought our game was interesting, and that the concept was engaging. This, more than anything else, gave us the motivation to aggressively pursue our final design goals and make them a reality.\\